import os
import sys
import shutil  # æ–°å¢å¯¼å…¥shutilæ¨¡å—
import json
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# import pyrealsense2 as rs
import numpy as np
import cv2
import glob
import trimesh
import logging
import torch
from PIL import Image
from omegaconf import OmegaConf
from hydra import initialize, compose
from hydra.utils import instantiate
import argparse

from Instance_Segmentation_Model.model.utils import Detections, convert_npz_to_json
from Instance_Segmentation_Model.model.loss import Similarity
from Instance_Segmentation_Model.utils.inout import save_json_bop23, load_json
from Instance_Segmentation_Model.utils.bbox_utils import CropResizePad
from Instance_Segmentation_Model.utils.poses.pose_utils import get_obj_poses_from_template_level, \
    load_index_level_in_level2
from Instance_Segmentation_Model.segment_anything.utils.amg import rle_to_mask
from Foundationpose.FoundationposeAPI import PoseEstimationAPI as FoundationposeProcessor
import time as tim


# è£…é¥°å™¨
def timed(task_name="ä»»åŠ¡"):
    def decorator(func):
        def wrapper(*args, **kwargs):
            print(f"{task_name}å¼€å§‹...", end='', flush=True)
            start_time = tim.time()
            result = func(*args, **kwargs)
            elapsed_time = tim.time() - start_time
            print(f"å®Œæˆ! è€—æ—¶: {elapsed_time:.2f}ç§’")
            return result
        return wrapper
    return decorator


# ä½¿ç”¨è£…é¥°å™¨
@timed("åˆå§‹åŒ–SAM")
def create_sam_processor(cad_path, sam_tmp_dir, segmentor_model, stability_score_thresh):
    return SamProcessor(cad_path, sam_tmp_dir, segmentor_model, stability_score_thresh)

@timed("åˆå§‹åŒ–FP")
def create_fp_processor(model_path, camera_info):
    return FoundationposeProcessor(model_path, camera_info)

# å°è£…è°ƒç”¨æ–¹æ³•å¹¶ç»Ÿè®¡è€—æ—¶
@timed("SAMåˆ‡å‰²")
def sam_process_frame(processor, rgb_path, depth_path, mask_path, cam_info):
    return processor.process_frame(rgb_path, depth_path, mask_path, cam_info)

@timed("FPä½å§¿ä¼°è®¡")  # è¿™é‡Œä½¿ç”¨è£…é¥°å™¨å¯¹æ–¹æ³•è€—æ—¶è¿›è¡Œç»Ÿè®¡
def fp_process_frame(processor, color_img, depth_map, mask, obj_id, frame_id, camera_matrix):
    return processor.process_frame(
        color_img=color_img,
        depth_map=depth_map,
        mask=mask,
        obj_id=obj_id,
        frame_id=frame_id,
        camera_matrix=camera_matrix
    )


def get_mesh_obj(mesh_dir: str):
    # mesh = trimesh.load(os.path.abspath(f'{ref_view_dir}/model/model.obj'))
    # mesh_file = self.get_gt_mesh_file(ob_id)
    mesh = trimesh.load(mesh_dir)
    mesh.vertices *= 1e-3     # è¿™é‡Œä¸ºä»€ä¹ˆè¦ä¹˜1e-3
    # æ£€æŸ¥æ˜¯å¦åŠ è½½äº†çº¹ç†
    if hasattr(mesh.visual, 'material') and mesh.visual.material:
        print("çº¹ç†åŠ è½½æˆåŠŸï¼")
        print("çº¹ç†æè´¨ä¿¡æ¯:", mesh.visual.material)
    else:
        print("çº¹ç†æœªåŠ è½½ã€‚")
    return mesh


def visualize_mask(detections, save_path="mask.png"):
    best_score = 0.0
    best_det = None
    # éå† detections.detections è€Œé detections å¯¹è±¡
    for det in detections:
        if best_score < det['score']:
            best_score = det['score']
            best_det = det
    if best_det is None:
        raise ValueError("No detections found!")
    mask = rle_to_mask(best_det["segmentation"])
    mask_img = Image.fromarray((mask * 255).astype(np.uint8))
    mask_img.save(save_path)
    return best_score


def load_camera_info(file_path):
    """
    è¯»å–ç›¸æœºå†…å‚çš„JSONæ–‡ä»¶ï¼Œè¿”å›æ ¼å¼åŒ–åçš„cam_infoå­—å…¸ã€‚
    
    :param file_path: JSONæ–‡ä»¶çš„è·¯å¾„
    :return: åŒ…å«ç›¸æœºå†…å‚çš„å­—å…¸
    """
    # try:
    with open(file_path, 'r') as file:
        data = json.load(file)
    
    # print(f"cam_info:{data}")

    cam_info = {}
    # ä»JSONä¸­æå–ç›¸å…³çš„cam_Kå’Œdepth_scale
    cam_info['cam_K'] = data['0']['cam_K']
    cam_info['depth_scale'] = data['0']['depth_scale']

    # cam_info['cam_K'] = data['cam_K']
    # cam_info['depth_scale'] = data['depth_scale']

    # print(cam_info)

    return cam_info

    # except Exception as e:
    #     print(f"No CAMERA INFO: {e}")
    #     return None


# ä¿å­˜ pose åˆ° txt æ–‡ä»¶
def save_pose(pose, filename):
    """
    å°†ä½å§¿çŸ©é˜µä¿å­˜åˆ° txt æ–‡ä»¶ã€‚
    Args:
        pose: å˜æ¢çŸ©é˜µ (numpy array, shape 4x4)
        filename: è¾“å‡ºçš„ txt æ–‡ä»¶å
    """
    if isinstance(pose, np.ndarray) and pose.shape == (4, 4):
        np.savetxt(filename, pose, fmt='%.8f', delimiter=' ')
        print(f"ä½å§¿å·²ä¿å­˜")
    else:
        raise ValueError("Pose must be a 4x4 NumPy array.")
    

class SamProcessor:
    def __init__(self, cad_path, sam_tmp_dir, segmentor_model, stability_score_thresh=0.97):
        # self.device = torch.device("cpu")   # æ˜¾å­˜ä¸å¤Ÿçš„è‹¦ğŸ˜
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # åŠ¨æ€åŠ è½½æ¨¡å‹é…ç½®
        with initialize(version_base=None, config_path="Instance_Segmentation_Model/configs"):
            cfg = compose(config_name='run_inference.yaml')
        with initialize(version_base=None, config_path="Instance_Segmentation_Model/configs/model"):
            if segmentor_model == "sam":
                cfg.model = compose(config_name='ISM_sam.yaml')
                cfg.model.segmentor_model.stability_score_thresh = stability_score_thresh
            elif segmentor_model == "fastsam":
                cfg.model = compose(config_name='ISM_fastsam.yaml')
            else:
                raise ValueError(f"Unsupported segmentor_model: {segmentor_model}")

        self.model = instantiate(cfg.model)
        self.model.descriptor_model.model = self.model.descriptor_model.model.to(self.device)
        self.model.descriptor_model.model.device = self.device

        if hasattr(self.model.segmentor_model, "predictor"):
            self.model.segmentor_model.predictor.model = (
                self.model.segmentor_model.predictor.model.to(self.device)
            )
        else:
            self.model.segmentor_model.model.setup_model(device=self.device, verbose=True)

        # self.model.segmentor_model.model.setup_model(device=self.device, verbose=True)

        # åŠ è½½CADå’Œåˆå§‹åŒ–æ¨¡æ¿
        self.mesh = get_mesh_obj(cad_path)     # ç†è®ºä¸Šæ¥è¯´åº”è¯¥æ˜¯è¦ä¹˜ä¸Šç³»æ•°çš„
        self.sam_tmp_dir = sam_tmp_dir
        os.makedirs(f"{self.sam_tmp_dir}/sam6d_results", exist_ok=True)
        self._init_template_data()
        self._init_pointcloud_and_poses()

    def _init_template_data(self):
        template_dir = os.path.join(self.sam_tmp_dir, 'templates')
        num_templates = len(glob.glob(f"{template_dir}/*.npy"))
        boxes, masks, templates = [], [], []
        for idx in range(num_templates):
            image = Image.open(os.path.join(template_dir, f'rgb_{idx}.png'))
            mask = Image.open(os.path.join(template_dir, f'mask_{idx}.png'))
            boxes.append(mask.getbbox())
            image = torch.from_numpy(np.array(image.convert("RGB")) / 255).float()
            mask = torch.from_numpy(np.array(mask.convert("L")) / 255).float()
            image = image * mask[:, :, None]
            templates.append(image)
            masks.append(mask.unsqueeze(-1))

        templates = torch.stack(templates).permute(0, 3, 1, 2)
        masks = torch.stack(masks).permute(0, 3, 1, 2)
        boxes = torch.tensor(np.array(boxes))
        proposal_processor = CropResizePad(224)
        templates = proposal_processor(images=templates, boxes=boxes).to(self.device)
        masks_cropped = proposal_processor(images=masks, boxes=boxes).to(self.device)

        self.model.ref_data = {
            "descriptors": self.model.descriptor_model.compute_features(templates, "x_norm_clstoken").unsqueeze(0).data,
            "appe_descriptors": self.model.descriptor_model.compute_masked_patch_feature(templates,
                                                                                         masks_cropped[:, 0, :,
                                                                                         :]).unsqueeze(0).data
        }

    # åœ¨RealTimeProcessorç±»ä¸­ä¿®æ”¹_init_pointcloud_and_posesæ–¹æ³•
    def _init_pointcloud_and_poses(self):
        # åˆå§‹åŒ–å§¿æ€æ•°æ®ï¼ˆå¼ºåˆ¶float32ï¼‰
        template_poses = get_obj_poses_from_template_level(level=2, pose_distribution="all")
        template_poses[:, :3, 3] *= 0.4
        self.model.ref_data["poses"] = torch.tensor(template_poses, dtype=torch.float32).to(self.device)

        # åˆå§‹åŒ–ç‚¹äº‘æ•°æ®ï¼ˆå¼ºåˆ¶float32ï¼‰
        model_points = self.mesh.sample(2048).astype(np.float32) / 1000.0
        self.model.ref_data["pointcloud"] = torch.tensor(model_points, dtype=torch.float32).unsqueeze(0).to(self.device)

    def process_frame(self, rgb_path, depth_path, mask_path, cam_info):
        # åŠ è½½rgb depth ç›¸æœºå†…å‚
        rgb = Image.open(rgb_path).convert("RGB")
        depth = np.array(Image.open(depth_path)).astype(np.int32)
        cam_K = np.array(cam_info['cam_K']).reshape((3, 3))
        depth_scale = np.array(cam_info['depth_scale'])

        # batchä¸ºå­—å…¸ï¼Œå­˜å‚¨ç»è¿‡è½¬åŒ–åçš„å¼ é‡ï¼Œæ–¹ä¾¿åç»­å¤„ç†
        batch = {
            "depth": torch.from_numpy(depth).float().unsqueeze(0).to(self.device),  # æ·»åŠ .float()
            "cam_intrinsic": torch.from_numpy(cam_K).float().unsqueeze(0).to(self.device),
            "depth_scale": torch.from_numpy(depth_scale).float().unsqueeze(0).to(self.device)
        }

        detections = self.model.segmentor_model.generate_masks(np.array(rgb))   # è°ƒç”¨æ¨¡å‹ï¼Œä»RGBå›¾åƒç”Ÿæˆåˆ†å‰²æ©ç 
        detections = Detections(detections) # å°†åˆ†å‰²æ¨¡å‹è½¬æ¢ä¸ºDetectionså¯¹è±¡ï¼ˆDetectionsç±»ï¼šä¸“é—¨å°è£…æ©ç ã€è¾¹ç•Œæ¡†ã€æ ‡ç­¾ç­‰ä¿¡æ¯ï¼‰

        query_decriptors, query_appe_descriptors = self.model.descriptor_model.forward(np.array(rgb), detections)   # æå–ç›®æ ‡çš„ç‰¹å¾æè¿°ç¬¦ã€‚decriptorsä¸ºå‡ ä½•ä¿¡æ¯ï¼Œappe+descriptorsä¸ºå¤–è§‚ä¿¡æ¯

        # è®¡ç®—è¯­ä¹‰åˆ†æ•°
        idx_selected_proposals, pred_idx_objects, semantic_score, best_template = self.model.compute_semantic_score(
            query_decriptors)

        # æƒç›Šä¹‹æŠ€ï¼šå½“è§†ç•Œæ— æ³•åˆ†å‰²æ—¶ï¼Œè®¡ç®—å‡ ä½•åˆ†æ•°ä¼šæœ‰bug
        if idx_selected_proposals.shape[0] == 1:
            print("åˆ†å‰²å¤±è´¥ï¼šidx_selected_proposals.shape[0]==1")
            return
        elif idx_selected_proposals.shape[0] == 0:
            print("åˆ†å‰²å¤±è´¥ï¼šidx_selected_proposals.shape[0]==0")
            return

        detections.filter(idx_selected_proposals)
        query_appe_descriptors = query_appe_descriptors[idx_selected_proposals, :]

        # è®¡ç®—å¤–è§‚åˆ†æ•°
        appe_scores, ref_aux_descriptor = self.model.compute_appearance_score(best_template, pred_idx_objects,
                                                                              query_appe_descriptors)

        # è®¡ç®—å‡ ä½•åˆ†æ•°
        image_uv = self.model.project_template_to_image(best_template, pred_idx_objects, batch, detections.masks)
        geometric_score, visible_ratio = self.model.compute_geometric_score(
            image_uv, detections, query_appe_descriptors, ref_aux_descriptor, self.model.visible_thred
        )

        # ç»¼åˆåˆ†æ•°è®¡ç®—
        final_score = (semantic_score + appe_scores + geometric_score * visible_ratio) / (1 + 1 + visible_ratio)
        detections.add_attribute("scores", final_score)
        detections.add_attribute("object_ids", torch.zeros_like(final_score))
        detections.to_numpy()

        # ä¿å­˜detectionså¯¹è±¡
        save_path = f"{self.sam_tmp_dir}/sam6d_results/detection_ism"
        detections.save_to_file(0, 0, 0, save_path, "Custom", return_results=False)
        detections = convert_npz_to_json(idx=0, list_npz_paths=[save_path + ".npz"])
        save_json_bop23(save_path + ".json", detections)

        # ä¿å­˜æ©ç å›¾
        # mask_path = f"{data_dir}/mask/mask_{os.path.basename(rgb_path)}"
        best_score = visualize_mask(detections, save_path=mask_path)
        print(f"æ©ç ä¿å­˜è‡³{mask_path};å…¶åˆ†æ•°ä¸º{best_score}", end='\t')


def capture_and_process(cad_path, sam_tmp_dir, data_dir, track_vis_dir, track_pose_dir, segmentor_model, stability_score_thresh, use_parallel_mode):
    # æ¸…ç©ºå¹¶åˆ›å»ºè¾“å‡ºç›®å½•
    rgb_dir = os.path.join(data_dir, "rgb")
    depth_dir = os.path.join(data_dir, "depth")
    mask_dir = os.path.join(data_dir, "mask")
    # track_vis_dir = os.path.join(data_dir, "track")
    camera_path = os.path.join(data_dir, 'scene_camera.json')

    # # åˆ é™¤æ—§ç›®å½•å¹¶é‡æ–°åˆ›å»º
    for dir_path in [mask_dir, track_vis_dir, track_pose_dir]:
        if os.path.exists(dir_path):
            shutil.rmtree(dir_path)  # åˆ é™¤ç›®å½•åŠå…¶å†…å®¹
        os.makedirs(dir_path, exist_ok=True)  # é‡æ–°åˆ›å»ºç©ºç›®å½•
        print(f"å·²æ¸…ç©ºè¾“å‡ºç›®å½•{dir_path}")

    # ç›¸æœºå†…å‚
    # print(camera_path)
    cam_info = load_camera_info(camera_path)
    # cam_info = {
    #     "cam_K": [608.190673828125, 0, 326.26312255859375, 0, 608.312744140625, 238.7494659423828, 0, 0, 1],
    #     "depth_scale": 0.001  # ä¿å­˜çš„å•ä½æ˜¯m(æš‚æ—¶ä¸å·²çŸ¥pipelineç»Ÿä¸€ï¼ŒåæœŸå¯ä¿®æ”¹)
    # }

    frame_count = 0  # åˆå§‹åŒ–è®¡æ•°å™¨

    # åˆå§‹åŒ–SAMå¤„ç†å®ä¾‹
    sam_processor = create_sam_processor(cad_path, sam_tmp_dir, segmentor_model, stability_score_thresh)

    # åˆå§‹åŒ–foundationå¤„ç†å®ä¾‹
    fp_processor = create_fp_processor(model_path=cad_path, camera_info=cam_info)

    # get the len of rgb
    files = os.listdir(rgb_dir)
    num_rgb = len(files)
    print(f"å½“è½®æ•°æ®é›†æ€»æ•°ä¸º{num_rgb}")

    while frame_count < num_rgb:
        # # ä¿å­˜å¸§åˆ°æŒ‡å®šæ–‡ä»¶
        rgb_path = os.path.join(rgb_dir, f"{str(frame_count).zfill(6)}.png")
        depth_path = os.path.join(depth_dir, f"{str(frame_count).zfill(6)}.png")
        mask_path = os.path.join(mask_dir, f"{str(frame_count).zfill(6)}.png")

        # ************ç”¨samè¿›è¡Œåˆ†å‰²************ #
        sam_process_frame(processor=sam_processor, rgb_path=rgb_path, depth_path=depth_path, mask_path=mask_path,
                                    cam_info=cam_info)  # å°†è¯»å–åˆ°çš„ç›¸æœºå‚æ•°ä¼ è¾“å…¥sam6dçš„åˆ†å‰²æ–‡ä»¶

        # ************ç”¨fpè¿›è¡Œç›®æ ‡è·Ÿè¸ª************ #

        color_bgr = cv2.imread(rgb_path)  # cv2é»˜è®¤è¯»å–çš„æ˜¯bgrå›¾åƒ
        color_img = cv2.cvtColor(color_bgr, cv2.COLOR_BGR2RGB)
        depth_map = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        if mask is None:
            frame_count += 1
            print(f"åˆ†å‰²å¤±è´¥ï¼Œå½“å‰å¸§{frame_count}è·³è¿‡")
            continue  # samåˆ†å‰²å¤±è´¥ï¼Œç›´æ¥è·³è¿‡
        
        pose, time = fp_process_frame(
            processor=fp_processor,
            color_img=color_img,
            depth_map=depth_map,
            mask=mask,
            obj_id=1,
            frame_id=frame_count,
            camera_matrix=fp_processor.config['camera_matrix']
        )
        
        # FP save
        vis_img = fp_processor.visualize_result(color_img, pose, fp_processor.config['camera_matrix'])
        cv2.imwrite(os.path.join(track_vis_dir, f'{frame_count}.png') , vis_img[..., ::-1])
        # print(f"æ£€æµ‹å¸§{frame_count}.pngå·²ä¿å­˜è‡³{track_vis_dir}")
        print(f"æ£€æµ‹å¸§{frame_count}.pngå·²ä¿å­˜", end="   ")
        
        # pose save
        save_pose(pose, f"{track_pose_dir}/pose_{frame_count}.txt")
        # # cv2.imshow("Visualization Result", vis_img[..., ::-1])      #å°†RGBå˜æˆBGR,CV2çš„é»˜è®¤å¤„ç†å½¢å¼
        # if cv2.waitKey(1) & 0xFF == ord('q'):
        #     break
        # print("é‡Šæ”¾FP......")

        frame_count += 1

        # SAMå¯è§†åŒ–ç»“æœ
        # cv2.imshow("Preview", np.asanyarray(color_frame.get_data()))
        # if cv2.waitKey(1) & 0xFF == ord('q'):
        #     break


if __name__ == "__main__":
    logging.basicConfig(level=logging.CRITICAL)

    # MASK DEBUG
    cup = 1
    time = 0
    parser = argparse.ArgumentParser()
    parser.add_argument("--cad_path", default=f"Data/real_data/cup{cup}_mesh/cup{cup}.obj", help="Path to CAD model")  # æ³¨æ„è¾“å…¥å•ä½ä¸ºmmï¼Œä¸­é—´å¤„ç†ä¸ºm
    parser.add_argument("--sam_tmp_dir", default=f"Data/real_data/cup{cup}_mesh/cup{cup}_tmp", help="tmp directory")
    parser.add_argument("--data_dir", default=f"Data/real_data/sam_test", help="rgb, depth and mask")
    parser.add_argument("--track_vis_dir", default=f"Data/real_data/sam_test/track_vis_cup{cup}", help="rgb, depth and mask")
    parser.add_argument("--track_pose_dir", default=f"Data/real_data/pour_water/sam_test/track_pose_cup{cup}", help="rgb, depth and mask")
    parser.add_argument("--segmentor_model", choices=["sam", "fastsam"], default="sam", help="Segmentor model type")
    parser.add_argument("--stability_score_thresh", type=float, default=0.97, help="SAM stability score threshold")
    args = parser.parse_args()

    # è°ƒæ•´æ—¥å¿—è¾“å‡ºç­‰çº§ï¼ˆæ­£å¸¸è¿è¡Œæ—¶ï¼‰
    logging.disable(logging.CRITICAL)
    
    capture_and_process(
        cad_path=args.cad_path,
        sam_tmp_dir=args.sam_tmp_dir,
        segmentor_model=args.segmentor_model,
        stability_score_thresh=args.stability_score_thresh,
        data_dir=args.data_dir,
        track_vis_dir=args.track_vis_dir,
        track_pose_dir=args.track_pose_dir,
        use_parallel_mode=False
    )

    # # record
    # cup = 0
    # time = 0

    # parser = argparse.ArgumentParser()
    # parser.add_argument("--cad_path", default=f"Data/real_data/cup{cup}_mesh/cup{cup}.obj", help="Path to CAD model")  # æ³¨æ„è¾“å…¥å•ä½ä¸ºmmï¼Œä¸­é—´å¤„ç†ä¸ºm
    # parser.add_argument("--sam_tmp_dir", default=f"Data/real_data/cup{cup}_mesh/cup{cup}_tmp", help="tmp directory")
    # parser.add_argument("--data_dir", default=f"Data/real_data/pour_water/episode_{time}", help="rgb, depth and mask")
    # parser.add_argument("--track_vis_dir", default=f"Data/real_data/pour_water/episode_{time}/track_vis_cup{cup}", help="rgb, depth and mask")
    # parser.add_argument("--track_pose_dir", default=f"Data/real_data/pour_water/episode_{time}/track_pose_cup{cup}", help="rgb, depth and mask")
    # parser.add_argument("--segmentor_model", choices=["sam", "fastsam"], default="sam", help="Segmentor model type")
    # parser.add_argument("--stability_score_thresh", type=float, default=0.97, help="SAM stability score threshold")
    # args = parser.parse_args()

    
    # for time in range(0, 8):
    #     for cup in range(1, -1, -1):
    #         args.cad_path = f"Data/real_data/cup{cup}_mesh/cup{cup}.obj"
    #         args.sam_tmp_dir = f"Data/real_data/cup{cup}_mesh/cup{cup}_tmp"
    #         args.data_dir = f"Data/real_data/pour_water/episode_{time}"
    #         args.track_vis_dir = f"Data/real_data/pour_water/episode_{time}/track_vis_cup{cup}"
    #         args.track_pose_dir = f"Data/real_data/pour_water/episode_{time}/track_pose_cup{cup}"
    #         args.segmentor_model = "sam"
    #         args.stability_score_thresh = 0.97

    #         # è°ƒæ•´æ—¥å¿—è¾“å‡ºç­‰çº§ï¼ˆæ­£å¸¸è¿è¡Œæ—¶ï¼‰
    #         logging.disable(logging.CRITICAL)
 
    #         capture_and_process(
    #             cad_path=args.cad_path,
    #             sam_tmp_dir=args.sam_tmp_dir,
    #             segmentor_model=args.segmentor_model,
    #             stability_score_thresh=args.stability_score_thresh,
    #             data_dir=args.data_dir,
    #             track_vis_dir=args.track_vis_dir,
    #             track_pose_dir=args.track_pose_dir,
    #             use_parallel_mode=False
    #         )